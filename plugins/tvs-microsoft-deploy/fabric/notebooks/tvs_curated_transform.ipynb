{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "microsoft": {
      "language": "python",
      "ms_spell_check": { "ms_spell_check_language": "en" }
    },
    "trident": {
      "lakehouse": {
        "default_lakehouse_name": "lh-tvs-curated",
        "default_lakehouse_workspace_id": ""
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TVS Curated Transform Pipeline\n",
        "\n",
        "This notebook reads from `lh-tvs-operational` (raw Dataverse data), applies transformations,\n",
        "and writes curated analytics-ready tables to `lh-tvs-curated`.\n",
        "\n",
        "**Transformations:**\n",
        "- SCD Type 2 tracking for Accounts dimension\n",
        "- Deduplication and type casting\n",
        "- Time entry aggregation by account and month\n",
        "- Account health scoring\n",
        "\n",
        "**Schedule:** Daily at 4:00 AM UTC (after Paylocity sync at 2:00 AM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql.functions import (\n",
        "    col, lit, current_timestamp, sha2, concat_ws, when, coalesce,\n",
        "    row_number, lead, sum as spark_sum, count, avg, max as spark_max,\n",
        "    min as spark_min, datediff, months_between, round as spark_round,\n",
        "    date_format, to_date, year, month\n",
        ")\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, IntegerType, DoubleType,\n",
        "    BooleanType, DateType, TimestampType\n",
        ")\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Configuration\n",
        "OPERATIONAL_LAKEHOUSE = \"lh-tvs-operational\"\n",
        "CURATED_LAKEHOUSE = \"lh-tvs-curated\"\n",
        "OPERATIONAL_PATH = f\"abfss://{OPERATIONAL_LAKEHOUSE}@onelake.dfs.fabric.microsoft.com\"\n",
        "CURATED_PATH = f\"abfss://{CURATED_LAKEHOUSE}@onelake.dfs.fabric.microsoft.com\"\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "print(f\"Pipeline started at {datetime.utcnow().isoformat()}\")\n",
        "print(f\"Reading from: {OPERATIONAL_LAKEHOUSE}\")\n",
        "print(f\"Writing to: {CURATED_LAKEHOUSE}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Cell 2: Read operational data and apply deduplication / type casting ────\n",
        "\n",
        "# Read raw Dataverse tables from operational lakehouse\n",
        "df_accounts_raw = spark.read.format(\"delta\").load(f\"{OPERATIONAL_PATH}/Tables/dv_accounts\")\n",
        "df_contacts_raw = spark.read.format(\"delta\").load(f\"{OPERATIONAL_PATH}/Tables/dv_contacts\")\n",
        "df_subscriptions_raw = spark.read.format(\"delta\").load(f\"{OPERATIONAL_PATH}/Tables/dv_subscriptions\")\n",
        "df_tasks_raw = spark.read.format(\"delta\").load(f\"{OPERATIONAL_PATH}/Tables/dv_tasks\")\n",
        "df_time_entries_raw = spark.read.format(\"delta\").load(f\"{OPERATIONAL_PATH}/Tables/dv_timeentries\")\n",
        "df_deliverables_raw = spark.read.format(\"delta\").load(f\"{OPERATIONAL_PATH}/Tables/dv_deliverables\")\n",
        "\n",
        "print(f\"Raw counts:\")\n",
        "print(f\"  Accounts:      {df_accounts_raw.count()}\")\n",
        "print(f\"  Contacts:      {df_contacts_raw.count()}\")\n",
        "print(f\"  Subscriptions: {df_subscriptions_raw.count()}\")\n",
        "print(f\"  Tasks:         {df_tasks_raw.count()}\")\n",
        "print(f\"  Time Entries:  {df_time_entries_raw.count()}\")\n",
        "print(f\"  Deliverables:  {df_deliverables_raw.count()}\")\n",
        "\n",
        "# Deduplicate accounts by tvs_accountid, keeping latest version\n",
        "window_latest = Window.partitionBy(\"tvs_accountid\").orderBy(col(\"modifiedon\").desc())\n",
        "\n",
        "df_accounts_deduped = (\n",
        "    df_accounts_raw\n",
        "    .withColumn(\"_row_num\", row_number().over(window_latest))\n",
        "    .filter(col(\"_row_num\") == 1)\n",
        "    .drop(\"_row_num\")\n",
        ")\n",
        "\n",
        "# Cast types for clean schema\n",
        "df_accounts_clean = (\n",
        "    df_accounts_deduped\n",
        "    .withColumn(\"tvs_monthlyhours\", col(\"tvs_monthlyhours\").cast(IntegerType()))\n",
        "    .withColumn(\"tvs_tier\", col(\"tvs_tier\").cast(IntegerType()))\n",
        "    .withColumn(\"tvs_status\", col(\"tvs_status\").cast(IntegerType()))\n",
        "    .withColumn(\"tvs_onboarddate\", to_date(col(\"tvs_onboarddate\")))\n",
        ")\n",
        "\n",
        "# Deduplicate contacts\n",
        "window_contacts = Window.partitionBy(\"tvs_contactid\").orderBy(col(\"modifiedon\").desc())\n",
        "df_contacts_clean = (\n",
        "    df_contacts_raw\n",
        "    .withColumn(\"_row_num\", row_number().over(window_contacts))\n",
        "    .filter(col(\"_row_num\") == 1)\n",
        "    .drop(\"_row_num\")\n",
        ")\n",
        "\n",
        "print(f\"\\nAfter deduplication:\")\n",
        "print(f\"  Accounts: {df_accounts_clean.count()}\")\n",
        "print(f\"  Contacts: {df_contacts_clean.count()}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Cell 3: SCD Type 2 for Accounts Dimension ────────────────────────────\n",
        "\n",
        "# Generate hash of trackable columns for change detection\n",
        "scd_columns = [\"tvs_name\", \"tvs_industry\", \"tvs_tier\", \"tvs_status\", \"tvs_monthlyhours\"]\n",
        "\n",
        "df_accounts_hashed = df_accounts_clean.withColumn(\n",
        "    \"_row_hash\",\n",
        "    sha2(concat_ws(\"|\", *[coalesce(col(c).cast(StringType()), lit(\"NULL\")) for c in scd_columns]), 256)\n",
        ")\n",
        "\n",
        "# Try to load existing dimension; if first run, create empty structure\n",
        "try:\n",
        "    df_dim_existing = spark.read.format(\"delta\").load(f\"{CURATED_PATH}/Tables/dim_accounts\")\n",
        "    has_existing = True\n",
        "    print(f\"Existing dim_accounts rows: {df_dim_existing.count()}\")\n",
        "except Exception:\n",
        "    has_existing = False\n",
        "    print(\"No existing dim_accounts found, creating initial load.\")\n",
        "\n",
        "if has_existing:\n",
        "    # Find changed records: hash mismatch on active records\n",
        "    df_current = df_dim_existing.filter(col(\"is_current\") == True)\n",
        "    \n",
        "    df_changed = (\n",
        "        df_accounts_hashed.alias(\"new\")\n",
        "        .join(df_current.alias(\"old\"), col(\"new.tvs_accountid\") == col(\"old.tvs_accountid\"), \"left\")\n",
        "        .filter(\n",
        "            (col(\"old._row_hash\").isNull()) |  # New records\n",
        "            (col(\"new._row_hash\") != col(\"old._row_hash\"))  # Changed records\n",
        "        )\n",
        "        .select(\"new.*\")\n",
        "    )\n",
        "    \n",
        "    changed_count = df_changed.count()\n",
        "    print(f\"Changed/new accounts: {changed_count}\")\n",
        "    \n",
        "    if changed_count > 0:\n",
        "        # Close old records\n",
        "        changed_ids = [row.tvs_accountid for row in df_changed.select(\"tvs_accountid\").collect()]\n",
        "        \n",
        "        df_closed = (\n",
        "            df_dim_existing\n",
        "            .withColumn(\"is_current\",\n",
        "                when(col(\"tvs_accountid\").isin(changed_ids) & (col(\"is_current\") == True), lit(False))\n",
        "                .otherwise(col(\"is_current\"))\n",
        "            )\n",
        "            .withColumn(\"valid_to\",\n",
        "                when(col(\"tvs_accountid\").isin(changed_ids) & (col(\"valid_to\").isNull()), current_timestamp())\n",
        "                .otherwise(col(\"valid_to\"))\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        # Create new version records\n",
        "        df_new_versions = (\n",
        "            df_changed\n",
        "            .withColumn(\"valid_from\", current_timestamp())\n",
        "            .withColumn(\"valid_to\", lit(None).cast(TimestampType()))\n",
        "            .withColumn(\"is_current\", lit(True))\n",
        "        )\n",
        "        \n",
        "        df_dim_final = df_closed.unionByName(df_new_versions, allowMissingColumns=True)\n",
        "    else:\n",
        "        df_dim_final = df_dim_existing\n",
        "        print(\"No changes detected, skipping SCD update.\")\n",
        "else:\n",
        "    # Initial load: all records are current\n",
        "    df_dim_final = (\n",
        "        df_accounts_hashed\n",
        "        .withColumn(\"valid_from\", current_timestamp())\n",
        "        .withColumn(\"valid_to\", lit(None).cast(TimestampType()))\n",
        "        .withColumn(\"is_current\", lit(True))\n",
        "    )\n",
        "\n",
        "# Write dim_accounts\n",
        "df_dim_final.write.format(\"delta\").mode(\"overwrite\").save(f\"{CURATED_PATH}/Tables/dim_accounts\")\n",
        "print(f\"dim_accounts written: {df_dim_final.count()} total rows\")\n",
        "\n",
        "# Write dim_contacts (simple overwrite, no SCD)\n",
        "df_contacts_clean.write.format(\"delta\").mode(\"overwrite\").save(f\"{CURATED_PATH}/Tables/dim_contacts\")\n",
        "print(f\"dim_contacts written: {df_contacts_clean.count()} rows\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Cell 4: Time Entry Aggregation and Fact Tables ─────────────────────────\n",
        "\n",
        "# Fact: Time Entries (clean, with derived columns)\n",
        "df_time_facts = (\n",
        "    df_time_entries_raw\n",
        "    .withColumn(\"tvs_hours\", col(\"tvs_hours\").cast(DoubleType()))\n",
        "    .withColumn(\"tvs_date\", to_date(col(\"tvs_date\")))\n",
        "    .withColumn(\"year_month\", date_format(col(\"tvs_date\"), \"yyyy-MM\"))\n",
        "    .withColumn(\"year\", year(col(\"tvs_date\")))\n",
        "    .withColumn(\"month\", month(col(\"tvs_date\")))\n",
        "    .withColumn(\"is_billable\", col(\"tvs_billable\").cast(BooleanType()))\n",
        ")\n",
        "\n",
        "df_time_facts.write.format(\"delta\").mode(\"overwrite\").save(f\"{CURATED_PATH}/Tables/fact_time_entries\")\n",
        "print(f\"fact_time_entries written: {df_time_facts.count()} rows\")\n",
        "\n",
        "# Fact: Subscriptions\n",
        "df_sub_facts = (\n",
        "    df_subscriptions_raw\n",
        "    .withColumn(\"tvs_startdate\", to_date(col(\"tvs_startdate\")))\n",
        "    .withColumn(\"tvs_enddate\", to_date(col(\"tvs_enddate\")))\n",
        "    .withColumn(\"tvs_monthlyhours\", col(\"tvs_monthlyhours\").cast(IntegerType()))\n",
        "    .withColumn(\"tvs_monthlyprice\", col(\"tvs_monthlyprice\").cast(DoubleType()))\n",
        ")\n",
        "\n",
        "df_sub_facts.write.format(\"delta\").mode(\"overwrite\").save(f\"{CURATED_PATH}/Tables/fact_subscriptions\")\n",
        "print(f\"fact_subscriptions written: {df_sub_facts.count()} rows\")\n",
        "\n",
        "# Fact: Deliverables\n",
        "df_del_facts = (\n",
        "    df_deliverables_raw\n",
        "    .withColumn(\"tvs_completeddate\", to_date(col(\"tvs_completeddate\")))\n",
        "    .withColumn(\"tvs_status\", col(\"tvs_status\").cast(IntegerType()))\n",
        ")\n",
        "\n",
        "df_del_facts.write.format(\"delta\").mode(\"overwrite\").save(f\"{CURATED_PATH}/Tables/fact_deliverables\")\n",
        "print(f\"fact_deliverables written: {df_del_facts.count()} rows\")\n",
        "\n",
        "# ── Monthly Utilization Aggregate ────────────────────────────────────────\n",
        "\n",
        "# Join time entries with account subscriptions to calculate utilization\n",
        "df_monthly_util = (\n",
        "    df_time_facts\n",
        "    .filter(col(\"is_billable\") == True)\n",
        "    .groupBy(\n",
        "        col(\"tvs_taskid\").alias(\"task_id\"),\n",
        "        col(\"year_month\"),\n",
        "        col(\"year\"),\n",
        "        col(\"month\")\n",
        "    )\n",
        "    .agg(\n",
        "        spark_sum(\"tvs_hours\").alias(\"total_hours\"),\n",
        "        count(\"*\").alias(\"entry_count\"),\n",
        "        avg(\"tvs_hours\").alias(\"avg_hours_per_entry\"),\n",
        "        spark_min(\"tvs_date\").alias(\"first_entry_date\"),\n",
        "        spark_max(\"tvs_date\").alias(\"last_entry_date\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Join with tasks to get account-level aggregation\n",
        "df_task_accounts = df_tasks_raw.select(\n",
        "    col(\"tvs_taskid\").alias(\"task_id\"),\n",
        "    col(\"tvs_accountid\").alias(\"account_id\")\n",
        ")\n",
        "\n",
        "df_account_monthly = (\n",
        "    df_monthly_util\n",
        "    .join(df_task_accounts, \"task_id\", \"left\")\n",
        "    .groupBy(\"account_id\", \"year_month\", \"year\", \"month\")\n",
        "    .agg(\n",
        "        spark_sum(\"total_hours\").alias(\"billable_hours\"),\n",
        "        spark_sum(\"entry_count\").alias(\"total_entries\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Join with subscriptions to get utilization rate\n",
        "df_active_subs = df_sub_facts.filter(col(\"tvs_status\") == 100000001).select(\n",
        "    col(\"tvs_accountid\").alias(\"account_id\"),\n",
        "    col(\"tvs_monthlyhours\").alias(\"allotted_hours\")\n",
        ").dropDuplicates([\"account_id\"])\n",
        "\n",
        "df_utilization = (\n",
        "    df_account_monthly\n",
        "    .join(df_active_subs, \"account_id\", \"left\")\n",
        "    .withColumn(\"allotted_hours\", coalesce(col(\"allotted_hours\"), lit(0)))\n",
        "    .withColumn(\"utilization_pct\",\n",
        "        when(col(\"allotted_hours\") > 0,\n",
        "            spark_round(col(\"billable_hours\") / col(\"allotted_hours\") * 100, 2)\n",
        "        ).otherwise(lit(0))\n",
        "    )\n",
        "    .withColumn(\"over_allotment\", col(\"billable_hours\") > col(\"allotted_hours\"))\n",
        "    .withColumn(\"calculated_at\", current_timestamp())\n",
        ")\n",
        "\n",
        "df_utilization.write.format(\"delta\").mode(\"overwrite\").save(f\"{CURATED_PATH}/Tables/agg_monthly_utilization\")\n",
        "print(f\"agg_monthly_utilization written: {df_utilization.count()} rows\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Cell 5: Account Health Score ──────────────────────────────────────────\n",
        "\n",
        "# Health score based on: utilization, task completion rate, deliverable rate, recency\n",
        "\n",
        "today = datetime.utcnow().date()\n",
        "current_ym = today.strftime(\"%Y-%m\")\n",
        "\n",
        "# Latest utilization per account\n",
        "window_latest_util = Window.partitionBy(\"account_id\").orderBy(col(\"year_month\").desc())\n",
        "df_latest_util = (\n",
        "    df_utilization\n",
        "    .withColumn(\"_rn\", row_number().over(window_latest_util))\n",
        "    .filter(col(\"_rn\") == 1)\n",
        "    .select(\"account_id\", \"billable_hours\", \"allotted_hours\", \"utilization_pct\")\n",
        ")\n",
        "\n",
        "# Task completion rate per account\n",
        "df_task_metrics = (\n",
        "    df_tasks_raw\n",
        "    .groupBy(col(\"tvs_accountid\").alias(\"account_id\"))\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"total_tasks\"),\n",
        "        count(when(col(\"tvs_status\") == 100000004, True)).alias(\"completed_tasks\"),\n",
        "        count(when(col(\"tvs_status\") == 100000002, True)).alias(\"blocked_tasks\"),\n",
        "        count(when(\n",
        "            (col(\"tvs_status\").isin([100000000, 100000001])) & \n",
        "            (col(\"tvs_duedate\") < lit(str(today))),\n",
        "            True\n",
        "        )).alias(\"overdue_tasks\")\n",
        "    )\n",
        "    .withColumn(\"task_completion_rate\",\n",
        "        when(col(\"total_tasks\") > 0,\n",
        "            spark_round(col(\"completed_tasks\") / col(\"total_tasks\") * 100, 2)\n",
        "        ).otherwise(lit(0))\n",
        "    )\n",
        ")\n",
        "\n",
        "# Deliverable metrics\n",
        "df_del_metrics = (\n",
        "    df_deliverables_raw\n",
        "    .groupBy(col(\"tvs_accountid\").alias(\"account_id\"))\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"total_deliverables\"),\n",
        "        count(when(col(\"tvs_status\").isin([100000003, 100000004]), True)).alias(\"approved_delivered\")\n",
        "    )\n",
        "    .withColumn(\"deliverable_rate\",\n",
        "        when(col(\"total_deliverables\") > 0,\n",
        "            spark_round(col(\"approved_delivered\") / col(\"total_deliverables\") * 100, 2)\n",
        "        ).otherwise(lit(0))\n",
        "    )\n",
        ")\n",
        "\n",
        "# Compose health score (0-100)\n",
        "df_health = (\n",
        "    df_accounts_clean\n",
        "    .select(col(\"tvs_accountid\").alias(\"account_id\"), \"tvs_name\", \"tvs_tier\", \"tvs_status\")\n",
        "    .join(df_latest_util, \"account_id\", \"left\")\n",
        "    .join(df_task_metrics, \"account_id\", \"left\")\n",
        "    .join(df_del_metrics, \"account_id\", \"left\")\n",
        "    .withColumn(\"utilization_score\",\n",
        "        when(col(\"utilization_pct\").between(50, 120), lit(30))  # Healthy utilization\n",
        "        .when(col(\"utilization_pct\") > 120, lit(15))  # Over-utilized\n",
        "        .when(col(\"utilization_pct\").between(20, 50), lit(20))  # Under-utilized\n",
        "        .otherwise(lit(5))  # Very low engagement\n",
        "    )\n",
        "    .withColumn(\"task_score\",\n",
        "        spark_round(coalesce(col(\"task_completion_rate\"), lit(0)) * 0.3, 2)\n",
        "    )\n",
        "    .withColumn(\"deliverable_score\",\n",
        "        spark_round(coalesce(col(\"deliverable_rate\"), lit(0)) * 0.2, 2)\n",
        "    )\n",
        "    .withColumn(\"overdue_penalty\",\n",
        "        when(coalesce(col(\"overdue_tasks\"), lit(0)) > 3, lit(-10))\n",
        "        .when(coalesce(col(\"overdue_tasks\"), lit(0)) > 0, lit(-5))\n",
        "        .otherwise(lit(0))\n",
        "    )\n",
        "    .withColumn(\"health_score\",\n",
        "        spark_round(\n",
        "            col(\"utilization_score\") + col(\"task_score\") + col(\"deliverable_score\") + col(\"overdue_penalty\") + lit(20),\n",
        "            0\n",
        "        )\n",
        "    )\n",
        "    .withColumn(\"health_score\",\n",
        "        when(col(\"health_score\") > 100, lit(100))\n",
        "        .when(col(\"health_score\") < 0, lit(0))\n",
        "        .otherwise(col(\"health_score\"))\n",
        "    )\n",
        "    .withColumn(\"health_category\",\n",
        "        when(col(\"health_score\") >= 80, lit(\"Healthy\"))\n",
        "        .when(col(\"health_score\") >= 60, lit(\"Needs Attention\"))\n",
        "        .when(col(\"health_score\") >= 40, lit(\"At Risk\"))\n",
        "        .otherwise(lit(\"Critical\"))\n",
        "    )\n",
        "    .withColumn(\"calculated_at\", current_timestamp())\n",
        ")\n",
        "\n",
        "df_health.write.format(\"delta\").mode(\"overwrite\").save(f\"{CURATED_PATH}/Tables/agg_account_health\")\n",
        "\n",
        "# Summary\n",
        "print(f\"\\nagg_account_health written: {df_health.count()} accounts\")\n",
        "print(\"\\nHealth Distribution:\")\n",
        "df_health.groupBy(\"health_category\").count().show()\n",
        "print(f\"\\nPipeline completed at {datetime.utcnow().isoformat()}\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}
