{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "trident": {
      "lakehouse": {
        "default_lakehouse_name": "lh-a3-extract",
        "default_lakehouse_workspace_id": ""
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A3 Archive Validation\n",
        "\n",
        "Validates completeness and quality of data extracted from the legacy A3 Firebase system.\n",
        "Checks:\n",
        "- Row counts per collection vs expected minimums\n",
        "- Data quality: null rates, invalid values, date ranges\n",
        "- Referential integrity: brokers <-> commissions <-> carriers\n",
        "- Duplicate detection\n",
        "\n",
        "**Run after:** firebase-extract function completes\n",
        "**Output:** Validation report written to Files/validation-reports/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, lit, count, when, isnan, isnull, countDistinct,\n",
        "    sum as spark_sum, avg, min as spark_min, max as spark_max,\n",
        "    current_timestamp, length, regexp_extract\n",
        ")\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "LAKEHOUSE = \"lh-a3-extract\"\n",
        "BASE_PATH = f\"abfss://{LAKEHOUSE}@onelake.dfs.fabric.microsoft.com\"\n",
        "\n",
        "# Expected minimum row counts (from legacy A3 system inventory)\n",
        "EXPECTED_MINIMUMS = {\n",
        "    \"brokers\": 500,\n",
        "    \"commissions\": 10000,\n",
        "    \"carriers\": 200,\n",
        "    \"contacts\": 2000,\n",
        "    \"activities\": 5000\n",
        "}\n",
        "\n",
        "COLLECTIONS = list(EXPECTED_MINIMUMS.keys())\n",
        "validation_results = {}\n",
        "\n",
        "print(f\"A3 Archive Validation started at {datetime.utcnow().isoformat()}\")\n",
        "print(f\"Collections to validate: {COLLECTIONS}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Cell 2: Row Counts and Basic Statistics ────────────────────────────────\n",
        "\n",
        "dataframes = {}\n",
        "count_results = {}\n",
        "\n",
        "for collection in COLLECTIONS:\n",
        "    try:\n",
        "        df = spark.read.format(\"delta\").load(f\"{BASE_PATH}/Tables/{collection}\")\n",
        "        dataframes[collection] = df\n",
        "        \n",
        "        actual_count = df.count()\n",
        "        expected_min = EXPECTED_MINIMUMS[collection]\n",
        "        is_complete = actual_count >= expected_min\n",
        "        completeness_pct = round(actual_count / expected_min * 100, 2) if expected_min > 0 else 0\n",
        "        \n",
        "        count_results[collection] = {\n",
        "            \"actual_count\": actual_count,\n",
        "            \"expected_minimum\": expected_min,\n",
        "            \"is_complete\": is_complete,\n",
        "            \"completeness_pct\": completeness_pct,\n",
        "            \"column_count\": len(df.columns),\n",
        "            \"columns\": df.columns\n",
        "        }\n",
        "        \n",
        "        status = \"PASS\" if is_complete else \"FAIL\"\n",
        "        print(f\"  [{status}] {collection}: {actual_count:,} rows (expected >= {expected_min:,}, {completeness_pct}%)\")\n",
        "        print(f\"         Columns ({len(df.columns)}): {', '.join(df.columns[:8])}{'...' if len(df.columns) > 8 else ''}\")\n",
        "    except Exception as e:\n",
        "        count_results[collection] = {\n",
        "            \"actual_count\": 0,\n",
        "            \"expected_minimum\": EXPECTED_MINIMUMS[collection],\n",
        "            \"is_complete\": False,\n",
        "            \"completeness_pct\": 0,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "        print(f\"  [ERROR] {collection}: {e}\")\n",
        "\n",
        "validation_results[\"row_counts\"] = count_results\n",
        "\n",
        "total_rows = sum(r[\"actual_count\"] for r in count_results.values())\n",
        "total_pass = sum(1 for r in count_results.values() if r[\"is_complete\"])\n",
        "print(f\"\\nTotal rows extracted: {total_rows:,}\")\n",
        "print(f\"Collections passing minimum: {total_pass}/{len(COLLECTIONS)}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Cell 3: Data Quality Checks ──────────────────────────────────────────\n",
        "\n",
        "quality_results = {}\n",
        "\n",
        "# Define required fields per collection\n",
        "REQUIRED_FIELDS = {\n",
        "    \"brokers\": [\"_id\", \"name\"],\n",
        "    \"commissions\": [\"_id\", \"amount\"],\n",
        "    \"carriers\": [\"_id\", \"name\"],\n",
        "    \"contacts\": [\"_id\"],\n",
        "    \"activities\": [\"_id\"]\n",
        "}\n",
        "\n",
        "for collection, df in dataframes.items():\n",
        "    quality = {\"null_rates\": {}, \"issues\": []}\n",
        "    total = df.count()\n",
        "    \n",
        "    if total == 0:\n",
        "        quality[\"issues\"].append(\"Collection is empty\")\n",
        "        quality_results[collection] = quality\n",
        "        continue\n",
        "    \n",
        "    # Check null rates for all columns\n",
        "    for col_name in df.columns:\n",
        "        null_count = df.filter(isnull(col(col_name)) | (col(col_name) == \"\")).count()\n",
        "        null_rate = round(null_count / total * 100, 2)\n",
        "        quality[\"null_rates\"][col_name] = null_rate\n",
        "        \n",
        "        # Flag high null rates on required fields\n",
        "        if col_name in REQUIRED_FIELDS.get(collection, []) and null_rate > 1:\n",
        "            quality[\"issues\"].append(f\"Required field '{col_name}' has {null_rate}% null rate\")\n",
        "    \n",
        "    # Check for duplicate _id values\n",
        "    if \"_id\" in df.columns:\n",
        "        distinct_ids = df.select(\"_id\").distinct().count()\n",
        "        duplicates = total - distinct_ids\n",
        "        quality[\"duplicate_ids\"] = duplicates\n",
        "        if duplicates > 0:\n",
        "            dup_rate = round(duplicates / total * 100, 2)\n",
        "            quality[\"issues\"].append(f\"{duplicates} duplicate _id values ({dup_rate}%)\")\n",
        "    \n",
        "    # Check _extractedAt timestamp validity\n",
        "    if \"_extractedAt\" in df.columns:\n",
        "        min_ts = df.agg(spark_min(\"_extractedAt\")).collect()[0][0]\n",
        "        max_ts = df.agg(spark_max(\"_extractedAt\")).collect()[0][0]\n",
        "        quality[\"extraction_range\"] = {\"min\": str(min_ts), \"max\": str(max_ts)}\n",
        "    \n",
        "    quality[\"overall_status\"] = \"PASS\" if len(quality[\"issues\"]) == 0 else \"WARN\" if len(quality[\"issues\"]) <= 2 else \"FAIL\"\n",
        "    quality_results[collection] = quality\n",
        "    \n",
        "    status = quality[\"overall_status\"]\n",
        "    issue_count = len(quality[\"issues\"])\n",
        "    print(f\"  [{status}] {collection}: {issue_count} issue(s)\")\n",
        "    for issue in quality[\"issues\"]:\n",
        "        print(f\"         - {issue}\")\n",
        "\n",
        "validation_results[\"data_quality\"] = quality_results"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Cell 4: Referential Integrity Checks ─────────────────────────────────\n",
        "\n",
        "integrity_results = {}\n",
        "\n",
        "# Check: commissions -> brokers (broker_id / brokerId reference)\n",
        "if \"commissions\" in dataframes and \"brokers\" in dataframes:\n",
        "    df_comm = dataframes[\"commissions\"]\n",
        "    df_brokers = dataframes[\"brokers\"]\n",
        "    \n",
        "    # Find the broker reference column in commissions\n",
        "    broker_ref_cols = [c for c in df_comm.columns if \"broker\" in c.lower() and \"id\" in c.lower()]\n",
        "    if not broker_ref_cols:\n",
        "        broker_ref_cols = [c for c in df_comm.columns if \"broker\" in c.lower() and \"ref\" in c.lower()]\n",
        "    \n",
        "    if broker_ref_cols:\n",
        "        ref_col = broker_ref_cols[0]\n",
        "        broker_ids = set(row[0] for row in df_brokers.select(\"_id\").collect())\n",
        "        \n",
        "        total_comm = df_comm.count()\n",
        "        orphaned = df_comm.filter(~col(ref_col).isin(list(broker_ids))).count()\n",
        "        orphan_rate = round(orphaned / total_comm * 100, 2) if total_comm > 0 else 0\n",
        "        \n",
        "        integrity_results[\"commissions_to_brokers\"] = {\n",
        "            \"reference_column\": ref_col,\n",
        "            \"total_records\": total_comm,\n",
        "            \"orphaned_records\": orphaned,\n",
        "            \"orphan_rate_pct\": orphan_rate,\n",
        "            \"status\": \"PASS\" if orphan_rate < 5 else \"WARN\" if orphan_rate < 15 else \"FAIL\"\n",
        "        }\n",
        "        print(f\"  [{integrity_results['commissions_to_brokers']['status']}] commissions -> brokers: {orphaned} orphaned ({orphan_rate}%)\")\n",
        "    else:\n",
        "        print(\"  [SKIP] No broker reference column found in commissions\")\n",
        "\n",
        "# Check: commissions -> carriers (carrier_id / carrierId reference)\n",
        "if \"commissions\" in dataframes and \"carriers\" in dataframes:\n",
        "    df_comm = dataframes[\"commissions\"]\n",
        "    df_carriers = dataframes[\"carriers\"]\n",
        "    \n",
        "    carrier_ref_cols = [c for c in df_comm.columns if \"carrier\" in c.lower() and (\"id\" in c.lower() or \"ref\" in c.lower())]\n",
        "    \n",
        "    if carrier_ref_cols:\n",
        "        ref_col = carrier_ref_cols[0]\n",
        "        carrier_ids = set(row[0] for row in df_carriers.select(\"_id\").collect())\n",
        "        \n",
        "        total_comm = df_comm.count()\n",
        "        orphaned = df_comm.filter(~col(ref_col).isin(list(carrier_ids))).count()\n",
        "        orphan_rate = round(orphaned / total_comm * 100, 2) if total_comm > 0 else 0\n",
        "        \n",
        "        integrity_results[\"commissions_to_carriers\"] = {\n",
        "            \"reference_column\": ref_col,\n",
        "            \"total_records\": total_comm,\n",
        "            \"orphaned_records\": orphaned,\n",
        "            \"orphan_rate_pct\": orphan_rate,\n",
        "            \"status\": \"PASS\" if orphan_rate < 5 else \"WARN\" if orphan_rate < 15 else \"FAIL\"\n",
        "        }\n",
        "        print(f\"  [{integrity_results['commissions_to_carriers']['status']}] commissions -> carriers: {orphaned} orphaned ({orphan_rate}%)\")\n",
        "    else:\n",
        "        print(\"  [SKIP] No carrier reference column found in commissions\")\n",
        "\n",
        "# Check: contacts -> brokers\n",
        "if \"contacts\" in dataframes and \"brokers\" in dataframes:\n",
        "    df_contacts = dataframes[\"contacts\"]\n",
        "    \n",
        "    broker_ref_cols = [c for c in df_contacts.columns if \"broker\" in c.lower() and (\"id\" in c.lower() or \"ref\" in c.lower())]\n",
        "    \n",
        "    if broker_ref_cols:\n",
        "        ref_col = broker_ref_cols[0]\n",
        "        broker_ids = set(row[0] for row in df_brokers.select(\"_id\").collect())\n",
        "        \n",
        "        total_contacts = df_contacts.count()\n",
        "        orphaned = df_contacts.filter(~col(ref_col).isin(list(broker_ids))).count()\n",
        "        orphan_rate = round(orphaned / total_contacts * 100, 2) if total_contacts > 0 else 0\n",
        "        \n",
        "        integrity_results[\"contacts_to_brokers\"] = {\n",
        "            \"reference_column\": ref_col,\n",
        "            \"total_records\": total_contacts,\n",
        "            \"orphaned_records\": orphaned,\n",
        "            \"orphan_rate_pct\": orphan_rate,\n",
        "            \"status\": \"PASS\" if orphan_rate < 5 else \"WARN\" if orphan_rate < 15 else \"FAIL\"\n",
        "        }\n",
        "        print(f\"  [{integrity_results['contacts_to_brokers']['status']}] contacts -> brokers: {orphaned} orphaned ({orphan_rate}%)\")\n",
        "    else:\n",
        "        print(\"  [SKIP] No broker reference column found in contacts\")\n",
        "\n",
        "validation_results[\"referential_integrity\"] = integrity_results"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Cell 5: Validation Report Summary ────────────────────────────────────\n",
        "\n",
        "# Compute overall status\n",
        "all_statuses = []\n",
        "for section in [\"row_counts\", \"data_quality\", \"referential_integrity\"]:\n",
        "    section_data = validation_results.get(section, {})\n",
        "    for key, value in section_data.items():\n",
        "        if isinstance(value, dict):\n",
        "            status = value.get(\"overall_status\") or value.get(\"status\")\n",
        "            if value.get(\"is_complete\") is True:\n",
        "                status = \"PASS\"\n",
        "            elif value.get(\"is_complete\") is False:\n",
        "                status = \"FAIL\"\n",
        "            if status:\n",
        "                all_statuses.append(status)\n",
        "\n",
        "fail_count = all_statuses.count(\"FAIL\")\n",
        "warn_count = all_statuses.count(\"WARN\")\n",
        "pass_count = all_statuses.count(\"PASS\")\n",
        "\n",
        "if fail_count > 0:\n",
        "    overall_status = \"FAIL\"\n",
        "elif warn_count > 0:\n",
        "    overall_status = \"WARN\"\n",
        "else:\n",
        "    overall_status = \"PASS\"\n",
        "\n",
        "validation_results[\"summary\"] = {\n",
        "    \"overall_status\": overall_status,\n",
        "    \"checks_passed\": pass_count,\n",
        "    \"checks_warned\": warn_count,\n",
        "    \"checks_failed\": fail_count,\n",
        "    \"total_rows_extracted\": total_rows,\n",
        "    \"collections_validated\": len(COLLECTIONS),\n",
        "    \"validated_at\": datetime.utcnow().isoformat()\n",
        "}\n",
        "\n",
        "# Write validation report to Files\n",
        "report_timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "report_path = f\"{BASE_PATH}/Files/validation-reports/validation_{report_timestamp}.json\"\n",
        "\n",
        "# Convert to JSON and save\n",
        "report_json = json.dumps(validation_results, indent=2, default=str)\n",
        "report_df = spark.createDataFrame([(report_json,)], [\"report\"])\n",
        "report_df.coalesce(1).write.mode(\"overwrite\").text(report_path)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"A3 ARCHIVE VALIDATION REPORT\")\n",
        "print(f\"=\" * 60)\n",
        "print(f\"Overall Status: [{overall_status}]\")\n",
        "print(f\"Validated At:   {validation_results['summary']['validated_at']}\")\n",
        "print(f\"Total Rows:     {total_rows:,}\")\n",
        "print(f\"\")\n",
        "print(f\"Check Results:\")\n",
        "print(f\"  PASS: {pass_count}\")\n",
        "print(f\"  WARN: {warn_count}\")\n",
        "print(f\"  FAIL: {fail_count}\")\n",
        "print(f\"\")\n",
        "print(f\"Report saved to: {report_path}\")\n",
        "print(f\"=\" * 60)\n",
        "\n",
        "# Raise error if critical failures for pipeline alerting\n",
        "if overall_status == \"FAIL\":\n",
        "    print(\"\\nWARNING: Validation found FAILURES. Review report for details.\")\n",
        "    # Uncomment to fail pipeline on validation errors:\n",
        "    # raise Exception(f\"A3 Archive validation failed: {fail_count} check(s) failed\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}
