{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "trident": {
      "lakehouse": {
        "default_lakehouse_name": "lh-holdings-rollup",
        "default_lakehouse_workspace_id": ""
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Consolidated Holdings Rollup\n",
        "\n",
        "Reads from all entity lakehouses via OneLake shortcuts and creates consolidated\n",
        "holdings-level views:\n",
        "- Revenue rollup across all 5 entities\n",
        "- Headcount by entity\n",
        "- Cost allocation\n",
        "- Entity health metrics\n",
        "\n",
        "**Schedule:** Daily at 6:00 AM UTC (after entity pipelines complete)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, lit, current_timestamp, when, coalesce, count, sum as spark_sum,\n",
        "    avg, round as spark_round, to_date, year, month, date_format, current_date\n",
        ")\n",
        "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
        "from datetime import datetime\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "ROLLUP_LAKEHOUSE = \"lh-holdings-rollup\"\n",
        "BASE_PATH = f\"abfss://{ROLLUP_LAKEHOUSE}@onelake.dfs.fabric.microsoft.com\"\n",
        "\n",
        "# Entity workspace paths (via OneLake shortcuts in consolidated lakehouse)\n",
        "TVS_PATH = f\"{BASE_PATH}/Tables/tvs\"\n",
        "CONSULTING_PATH = f\"{BASE_PATH}/Tables/consulting\"\n",
        "PLATFORM_PATH = f\"{BASE_PATH}/Tables/platform\"\n",
        "MEDIA_PATH = f\"{BASE_PATH}/Tables/media\"\n",
        "A3_PATH = f\"{BASE_PATH}/Tables/a3\"\n",
        "\n",
        "ENTITIES = [\"TAIA\", \"TVS\", \"Lobbi Consulting\", \"Medicare Consulting\", \"Media Company\"]\n",
        "\n",
        "print(f\"Consolidated rollup started at {datetime.utcnow().isoformat()}\")\n",
        "print(f\"Reading from {ROLLUP_LAKEHOUSE} shortcuts\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Cell 2: Revenue Rollup ───────────────────────────────────────────────\n",
        "\n",
        "from functools import reduce\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "revenue_frames = []\n",
        "\n",
        "# TVS Revenue: Monthly subscription revenue from active subscriptions\n",
        "try:\n",
        "    df_tvs_subs = spark.read.format(\"delta\").load(f\"{TVS_PATH}/fact_subscriptions\")\n",
        "    df_tvs_revenue = (\n",
        "        df_tvs_subs\n",
        "        .filter(col(\"tvs_status\") == 100000001)  # Active\n",
        "        .withColumn(\"monthly_revenue\", col(\"tvs_monthlyprice\").cast(DoubleType()))\n",
        "        .agg(spark_sum(\"monthly_revenue\").alias(\"monthly_revenue\"))\n",
        "        .withColumn(\"entity\", lit(\"TVS\"))\n",
        "        .withColumn(\"revenue_type\", lit(\"Subscription\"))\n",
        "        .withColumn(\"annual_projected\", col(\"monthly_revenue\") * 12)\n",
        "    )\n",
        "    revenue_frames.append(df_tvs_revenue)\n",
        "    print(f\"TVS subscription data loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"TVS data not available: {e}\")\n",
        "\n",
        "# Consulting Revenue: Engagement values\n",
        "try:\n",
        "    df_consulting_eng = spark.read.format(\"delta\").load(f\"{CONSULTING_PATH}/dv_engagements\")\n",
        "    df_consulting_accts = spark.read.format(\"delta\").load(f\"{CONSULTING_PATH}/dv_accounts\")\n",
        "    \n",
        "    # Revenue by consulting entity\n",
        "    for entity_code, entity_name in [(100000000, \"Lobbi Consulting\"), (100000001, \"Medicare Consulting\")]:\n",
        "        entity_accounts = df_consulting_accts.filter(col(\"tvs_entity\") == entity_code).select(\"tvs_consultingaccountid\")\n",
        "        df_entity_rev = (\n",
        "            df_consulting_eng\n",
        "            .join(entity_accounts, df_consulting_eng[\"tvs_accountid\"] == entity_accounts[\"tvs_consultingaccountid\"])\n",
        "            .filter(col(\"tvs_status\").isin([100000002, 100000004]))  # Active or Completed\n",
        "            .withColumn(\"value\", col(\"tvs_value\").cast(DoubleType()))\n",
        "            .withColumn(\"invoiced\", col(\"tvs_invoicedamount\").cast(DoubleType()))\n",
        "            .agg(\n",
        "                spark_sum(\"value\").alias(\"total_contract_value\"),\n",
        "                spark_sum(\"invoiced\").alias(\"total_invoiced\")\n",
        "            )\n",
        "            .withColumn(\"entity\", lit(entity_name))\n",
        "            .withColumn(\"revenue_type\", lit(\"Engagement\"))\n",
        "            .withColumn(\"monthly_revenue\", col(\"total_invoiced\") / 12)\n",
        "            .withColumn(\"annual_projected\", col(\"total_contract_value\"))\n",
        "        )\n",
        "        revenue_frames.append(df_entity_rev)\n",
        "    print(f\"Consulting data loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"Consulting data not available: {e}\")\n",
        "\n",
        "# TAIA / A3 Revenue: Historical commission data\n",
        "try:\n",
        "    df_a3_commissions = spark.read.format(\"delta\").load(f\"{A3_PATH}/commissions\")\n",
        "    df_taia_rev = (\n",
        "        df_a3_commissions\n",
        "        .withColumn(\"amount\", col(\"amount\").cast(DoubleType()))\n",
        "        .agg(spark_sum(\"amount\").alias(\"total_commissions\"))\n",
        "        .withColumn(\"entity\", lit(\"TAIA\"))\n",
        "        .withColumn(\"revenue_type\", lit(\"Commission\"))\n",
        "        .withColumn(\"monthly_revenue\", col(\"total_commissions\") / 12)\n",
        "        .withColumn(\"annual_projected\", col(\"total_commissions\"))\n",
        "    )\n",
        "    revenue_frames.append(df_taia_rev)\n",
        "    print(\"TAIA/A3 commission data loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"A3 data not available: {e}\")\n",
        "\n",
        "# Combine all revenue data\n",
        "if revenue_frames:\n",
        "    df_revenue_rollup = reduce(\n",
        "        lambda a, b: a.select(\"entity\", \"revenue_type\", \"monthly_revenue\", \"annual_projected\")\n",
        "            .unionByName(\n",
        "                b.select(\"entity\", \"revenue_type\", \"monthly_revenue\", \"annual_projected\"),\n",
        "                allowMissingColumns=True\n",
        "            ),\n",
        "        revenue_frames\n",
        "    ).withColumn(\"calculated_at\", current_timestamp())\n",
        "\n",
        "    df_revenue_rollup.write.format(\"delta\").mode(\"overwrite\").save(f\"{BASE_PATH}/Tables/rollup_revenue\")\n",
        "    print(f\"\\nRevenue rollup written: {df_revenue_rollup.count()} entity records\")\n",
        "    df_revenue_rollup.show(truncate=False)\n",
        "else:\n",
        "    print(\"No revenue data available for rollup\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Cell 3: Headcount and Cost Allocation ────────────────────────────────\n",
        "\n",
        "# Headcount from TVS time entries (unique VAs)\n",
        "headcount_data = []\n",
        "\n",
        "try:\n",
        "    df_tvs_time = spark.read.format(\"delta\").load(f\"{TVS_PATH}/fact_time_entries\")\n",
        "    tvs_active_vas = df_tvs_time.select(\"tvs_userid\").distinct().count()\n",
        "    tvs_total_hours = df_tvs_time.filter(col(\"is_billable\") == True).agg(spark_sum(\"tvs_hours\")).collect()[0][0] or 0\n",
        "    headcount_data.append((\"TVS\", \"Virtual Assistants\", tvs_active_vas, float(tvs_total_hours), 0.0))\n",
        "    print(f\"TVS: {tvs_active_vas} active VAs, {tvs_total_hours:.1f} total billable hours\")\n",
        "except Exception as e:\n",
        "    print(f\"TVS headcount not available: {e}\")\n",
        "\n",
        "# Consulting headcount from activities (unique owners)\n",
        "try:\n",
        "    df_consulting_acts = spark.read.format(\"delta\").load(f\"{CONSULTING_PATH}/dv_activities\")\n",
        "    consulting_staff = df_consulting_acts.select(\"tvs_owner\").distinct().count()\n",
        "    headcount_data.append((\"Lobbi Consulting\", \"Consultants\", consulting_staff, 0.0, 0.0))\n",
        "    headcount_data.append((\"Medicare Consulting\", \"Consultants\", max(1, consulting_staff // 2), 0.0, 0.0))\n",
        "    print(f\"Consulting: {consulting_staff} unique staff\")\n",
        "except Exception as e:\n",
        "    print(f\"Consulting headcount not available: {e}\")\n",
        "\n",
        "# A3/TAIA headcount from brokers\n",
        "try:\n",
        "    df_brokers = spark.read.format(\"delta\").load(f\"{A3_PATH}/brokers\")\n",
        "    broker_count = df_brokers.count()\n",
        "    headcount_data.append((\"TAIA\", \"Brokers\", broker_count, 0.0, 0.0))\n",
        "    print(f\"TAIA: {broker_count} brokers\")\n",
        "except Exception as e:\n",
        "    print(f\"TAIA headcount not available: {e}\")\n",
        "\n",
        "# Create headcount DataFrame\n",
        "if headcount_data:\n",
        "    df_headcount = (\n",
        "        spark.createDataFrame(headcount_data, [\"entity\", \"role_type\", \"headcount\", \"total_hours\", \"cost_allocated\"])\n",
        "        .withColumn(\"calculated_at\", current_timestamp())\n",
        "    )\n",
        "    \n",
        "    # Cost allocation: estimated based on entity type\n",
        "    df_cost = df_headcount.withColumn(\"estimated_monthly_cost\",\n",
        "        when(col(\"role_type\") == \"Virtual Assistants\", col(\"headcount\") * 800)  # VA avg cost\n",
        "        .when(col(\"role_type\") == \"Consultants\", col(\"headcount\") * 5000)  # Consultant avg cost\n",
        "        .when(col(\"role_type\") == \"Brokers\", col(\"headcount\") * 0)  # Commission-based\n",
        "        .otherwise(lit(0))\n",
        "    )\n",
        "    \n",
        "    df_cost.write.format(\"delta\").mode(\"overwrite\").save(f\"{BASE_PATH}/Tables/rollup_headcount\")\n",
        "    df_cost.write.format(\"delta\").mode(\"overwrite\").save(f\"{BASE_PATH}/Tables/rollup_cost_allocation\")\n",
        "    print(f\"\\nHeadcount & cost rollup written\")\n",
        "    df_cost.show(truncate=False)\n",
        "else:\n",
        "    print(\"No headcount data available\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Cell 4: Entity Health Metrics ─────────────────────────────────────────\n",
        "\n",
        "health_data = []\n",
        "\n",
        "# TVS Health: based on account health scores\n",
        "try:\n",
        "    df_tvs_health = spark.read.format(\"delta\").load(f\"{TVS_PATH}/agg_account_health\")\n",
        "    tvs_avg_health = df_tvs_health.agg(avg(\"health_score\")).collect()[0][0] or 0\n",
        "    tvs_total_accounts = df_tvs_health.count()\n",
        "    tvs_critical = df_tvs_health.filter(col(\"health_category\") == \"Critical\").count()\n",
        "    health_data.append((\"TVS\", float(tvs_avg_health), tvs_total_accounts, tvs_critical, \"Operational\"))\n",
        "    print(f\"TVS health: avg={tvs_avg_health:.1f}, accounts={tvs_total_accounts}, critical={tvs_critical}\")\n",
        "except Exception as e:\n",
        "    print(f\"TVS health data not available: {e}\")\n",
        "    health_data.append((\"TVS\", 0.0, 0, 0, \"No Data\"))\n",
        "\n",
        "# Consulting Health: based on engagement health\n",
        "try:\n",
        "    df_eng_health = spark.read.format(\"delta\").load(f\"{CONSULTING_PATH}/agg_engagement_health\")\n",
        "    for entity_name in [\"Lobbi Consulting\", \"Medicare Consulting\"]:\n",
        "        df_entity = df_eng_health.filter(col(\"entity_name\") == entity_name)\n",
        "        if df_entity.count() > 0:\n",
        "            ent_avg = df_entity.agg(avg(\"health_score\")).collect()[0][0] or 0\n",
        "            ent_total = df_entity.count()\n",
        "            ent_critical = df_entity.filter(col(\"health_label\") == \"Critical\").count()\n",
        "            health_data.append((entity_name, float(ent_avg), ent_total, ent_critical, \"Operational\"))\n",
        "        else:\n",
        "            health_data.append((entity_name, 0.0, 0, 0, \"No Data\"))\n",
        "    print(f\"Consulting health loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"Consulting health not available: {e}\")\n",
        "    health_data.append((\"Lobbi Consulting\", 0.0, 0, 0, \"No Data\"))\n",
        "    health_data.append((\"Medicare Consulting\", 0.0, 0, 0, \"No Data\"))\n",
        "\n",
        "# TAIA Health: based on A3 data completeness\n",
        "try:\n",
        "    df_brokers = spark.read.format(\"delta\").load(f\"{A3_PATH}/brokers\")\n",
        "    df_commissions = spark.read.format(\"delta\").load(f\"{A3_PATH}/commissions\")\n",
        "    broker_count = df_brokers.count()\n",
        "    commission_count = df_commissions.count()\n",
        "    taia_health = min(100, (broker_count + commission_count) / 10)  # Basic metric\n",
        "    health_data.append((\"TAIA\", float(taia_health), broker_count, 0, \"Archive\"))\n",
        "    print(f\"TAIA: {broker_count} brokers, {commission_count} commissions\")\n",
        "except Exception as e:\n",
        "    print(f\"TAIA data not available: {e}\")\n",
        "    health_data.append((\"TAIA\", 0.0, 0, 0, \"No Data\"))\n",
        "\n",
        "# Media Company placeholder\n",
        "health_data.append((\"Media Company\", 50.0, 0, 0, \"Setup\"))\n",
        "\n",
        "# Write entity health rollup\n",
        "df_entity_health = (\n",
        "    spark.createDataFrame(\n",
        "        health_data,\n",
        "        [\"entity\", \"avg_health_score\", \"total_items\", \"critical_items\", \"operational_status\"]\n",
        "    )\n",
        "    .withColumn(\"health_category\",\n",
        "        when(col(\"avg_health_score\") >= 75, lit(\"Healthy\"))\n",
        "        .when(col(\"avg_health_score\") >= 50, lit(\"Needs Attention\"))\n",
        "        .when(col(\"avg_health_score\") >= 25, lit(\"At Risk\"))\n",
        "        .otherwise(lit(\"Critical\"))\n",
        "    )\n",
        "    .withColumn(\"calculated_at\", current_timestamp())\n",
        ")\n",
        "\n",
        "df_entity_health.write.format(\"delta\").mode(\"overwrite\").save(f\"{BASE_PATH}/Tables/rollup_entity_health\")\n",
        "print(f\"\\nEntity health rollup written: {df_entity_health.count()} entities\")\n",
        "df_entity_health.show(truncate=False)\n",
        "print(f\"Consolidated rollup completed at {datetime.utcnow().isoformat()}\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}
